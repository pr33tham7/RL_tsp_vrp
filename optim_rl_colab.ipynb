{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pr33tham7/RL_tsp_vrp/blob/main/optim_rl_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6475985d",
      "metadata": {
        "id": "6475985d"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pr33tham7/RL_tsp_vrp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNKAwo2VGZdY",
        "outputId": "6fcfe39b-c988-47a2-b7d0-66ab06291044"
      },
      "id": "CNKAwo2VGZdY",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RL_tsp_vrp'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 41 (delta 6), reused 31 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (41/41), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "09c98831",
      "metadata": {
        "id": "09c98831"
      },
      "outputs": [],
      "source": [
        "\"\"\"Defines the main trainer model for combinatorial problems\n",
        "\n",
        "Each task must define the following functions:\n",
        "* mask_fn: can be None\n",
        "* update_fn: can be None\n",
        "* reward_fn: specifies the quality of found solutions\n",
        "* render_fn: Specifies how to plot found solutions. Can be None\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from RL_tsp_vrp import *\n",
        "from RL_tsp_vrp.model import DRL4TSP, Encoder\n",
        "from RL_tsp_vrp.tasks import tsp\n",
        "from RL_tsp_vrp.tasks.tsp import TSPDataset\n",
        "from RL_tsp_vrp.tasks import vrp\n",
        "from RL_tsp_vrp.tasks.vrp import VehicleRoutingDataset\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "\n",
        "\n",
        "class StateCritic(nn.Module):\n",
        "    \"\"\"Estimates the problem complexity.\n",
        "\n",
        "    This is a basic module that just looks at the log-probabilities predicted by\n",
        "    the encoder + decoder, and returns an estimate of complexity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, static_size, dynamic_size, hidden_size):\n",
        "        super(StateCritic, self).__init__()\n",
        "\n",
        "        self.static_encoder = Encoder(static_size, hidden_size)\n",
        "        self.dynamic_encoder = Encoder(dynamic_size, hidden_size)\n",
        "\n",
        "        # Define the encoder & decoder models\n",
        "        self.fc1 = nn.Conv1d(hidden_size * 2, 20, kernel_size=1)\n",
        "        self.fc2 = nn.Conv1d(20, 20, kernel_size=1)\n",
        "        self.fc3 = nn.Conv1d(20, 1, kernel_size=1)\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if len(p.shape) > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, static, dynamic):\n",
        "\n",
        "        # Use the probabilities of visiting each\n",
        "        static_hidden = self.static_encoder(static)\n",
        "        dynamic_hidden = self.dynamic_encoder(dynamic)\n",
        "\n",
        "        hidden = torch.cat((static_hidden, dynamic_hidden), 1)\n",
        "\n",
        "        output = F.relu(self.fc1(hidden))\n",
        "        output = F.relu(self.fc2(output))\n",
        "        output = self.fc3(output).sum(dim=2)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"Estimates the problem complexity.\n",
        "\n",
        "    This is a basic module that just looks at the log-probabilities predicted by\n",
        "    the encoder + decoder, and returns an estimate of complexity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Define the encoder & decoder models\n",
        "        self.fc1 = nn.Conv1d(1, hidden_size, kernel_size=1)\n",
        "        self.fc2 = nn.Conv1d(hidden_size, 20, kernel_size=1)\n",
        "        self.fc3 = nn.Conv1d(20, 1, kernel_size=1)\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if len(p.shape) > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        output = F.relu(self.fc1(input.unsqueeze(1)))\n",
        "        output = F.relu(self.fc2(output)).squeeze(2)\n",
        "        output = self.fc3(output).sum(dim=2)\n",
        "        return output\n",
        "\n",
        "\n",
        "def validate(data_loader, actor, reward_fn, render_fn=None, save_dir='.',\n",
        "             num_plot=5):\n",
        "    \"\"\"Used to monitor progress on a validation set & optionally plot solution.\"\"\"\n",
        "\n",
        "    actor.eval()\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    rewards = []\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        static, dynamic, x0 = batch\n",
        "\n",
        "        static = static.to(device)\n",
        "        dynamic = dynamic.to(device)\n",
        "        x0 = x0.to(device) if len(x0) > 0 else None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            tour_indices, _ = actor.forward(static, dynamic, x0)\n",
        "\n",
        "        reward = reward_fn(static, tour_indices).mean().item()\n",
        "        rewards.append(reward)\n",
        "\n",
        "        if render_fn is not None and batch_idx < num_plot:\n",
        "            name = 'batch%d_%2.4f.png'%(batch_idx, reward)\n",
        "            path = os.path.join(save_dir, name)\n",
        "            render_fn(static, tour_indices, path)\n",
        "\n",
        "    actor.train()\n",
        "    return np.mean(rewards)\n",
        "\n",
        "\n",
        "def train(actor, critic, task, num_nodes, train_data, valid_data, reward_fn,\n",
        "          render_fn, batch_size, actor_lr, critic_lr, max_grad_norm,\n",
        "          **kwargs):\n",
        "    \"\"\"Constructs the main actor & critic networks, and performs all training.\"\"\"\n",
        "\n",
        "    now = '%s' % datetime.datetime.now().time()\n",
        "    now = now.replace(':', '_')\n",
        "    save_dir = os.path.join(task, '%d' % num_nodes, now)\n",
        "\n",
        "    checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    actor_optim = optim.Adam(actor.parameters(), lr=actor_lr)\n",
        "    critic_optim = optim.Adam(critic.parameters(), lr=critic_lr)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size, True, num_workers=0)\n",
        "    valid_loader = DataLoader(valid_data, batch_size, False, num_workers=0)\n",
        "\n",
        "    best_params = None\n",
        "    best_reward = np.inf\n",
        "\n",
        "    for epoch in range(6):\n",
        "        print(epoch)\n",
        "        actor.train()\n",
        "        critic.train()\n",
        "\n",
        "        times, losses, rewards, critic_rewards = [], [], [], []\n",
        "\n",
        "        epoch_start = time.time()\n",
        "        start = epoch_start\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "            static, dynamic, x0 = batch\n",
        "\n",
        "            static = static.to(device)\n",
        "            dynamic = dynamic.to(device)\n",
        "            x0 = x0.to(device) if len(x0) > 0 else None\n",
        "\n",
        "            # Full forward pass through the dataset\n",
        "            tour_indices, tour_logp = actor(static, dynamic, x0)\n",
        "\n",
        "            # Sum the log probabilities for each city in the tour\n",
        "            reward = reward_fn(static, tour_indices)\n",
        "\n",
        "            # Query the critic for an estimate of the reward\n",
        "            critic_est = critic(static, dynamic).view(-1)\n",
        "\n",
        "            advantage = (reward - critic_est)\n",
        "            actor_loss = torch.mean(advantage.detach() * tour_logp.sum(dim=1))\n",
        "            critic_loss = torch.mean(advantage ** 2)\n",
        "\n",
        "            actor_optim.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(actor.parameters(), max_grad_norm)\n",
        "            actor_optim.step()\n",
        "\n",
        "            critic_optim.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(critic.parameters(), max_grad_norm)\n",
        "            critic_optim.step()\n",
        "\n",
        "            critic_rewards.append(torch.mean(critic_est.detach()).item())\n",
        "            rewards.append(torch.mean(reward.detach()).item())\n",
        "            losses.append(torch.mean(actor_loss.detach()).item())\n",
        "\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                end = time.time()\n",
        "                times.append(end - start)\n",
        "                start = end\n",
        "\n",
        "                mean_loss = np.mean(losses[-100:])\n",
        "                mean_reward = np.mean(rewards[-100:])\n",
        "\n",
        "                print('  Batch %d/%d, reward: %2.3f, loss: %2.4f, took: %2.4fs' %\n",
        "                      (batch_idx, len(train_loader), mean_reward, mean_loss,\n",
        "                       times[-1]))\n",
        "\n",
        "        mean_loss = np.mean(losses)\n",
        "        mean_reward = np.mean(rewards)\n",
        "\n",
        "        # Save the weights\n",
        "        epoch_dir = os.path.join(checkpoint_dir, '%s' % epoch)\n",
        "        if not os.path.exists(epoch_dir):\n",
        "            os.makedirs(epoch_dir)\n",
        "\n",
        "        save_path = os.path.join(epoch_dir, 'actor.pt')\n",
        "        torch.save(actor.state_dict(), save_path)\n",
        "\n",
        "        save_path = os.path.join(epoch_dir, 'critic.pt')\n",
        "        torch.save(critic.state_dict(), save_path)\n",
        "\n",
        "        # Save rendering of validation set tours\n",
        "        valid_dir = os.path.join(save_dir, '%s' % epoch)\n",
        "\n",
        "        mean_valid = validate(valid_loader, actor, reward_fn, render_fn,\n",
        "                              valid_dir, num_plot=5)\n",
        "\n",
        "        # Save best model parameters\n",
        "        if mean_valid < best_reward:\n",
        "\n",
        "            best_reward = mean_valid\n",
        "\n",
        "            save_path = os.path.join(save_dir, 'actor.pt')\n",
        "            torch.save(actor.state_dict(), save_path)\n",
        "\n",
        "            save_path = os.path.join(save_dir, 'critic.pt')\n",
        "            torch.save(critic.state_dict(), save_path)\n",
        "\n",
        "        print('Mean epoch loss/reward: %2.4f, %2.4f, %2.4f, took: %2.4fs '\\\n",
        "              '(%2.4fs / 100 batches)\\n' % \\\n",
        "              (mean_loss, mean_reward, mean_valid, time.time() - epoch_start,\n",
        "              np.mean(times)))\n",
        "\n",
        "\n",
        "\n",
        "def train_tsp():#args\n",
        "\n",
        "    # Goals from paper:\n",
        "    # TSP20, 3.97\n",
        "    # TSP50, 6.08\n",
        "    # TSP100, 8.44\n",
        "\n",
        "    # from RL_tsp_vrp.tasks import tsp\n",
        "    # from tasks.tsp import TSPDataset\n",
        "\n",
        "    STATIC_SIZE = 2 # (x, y)\n",
        "    DYNAMIC_SIZE = 1 # dummy for compatibility\n",
        "    \n",
        "    \n",
        "#  parser = argparse.ArgumentParser(description='Combinatorial Optimization')\n",
        "#     parser.add_argument('--seed', default=12345, type=int)\n",
        "#     parser.add_argument('--checkpoint', default=None)\n",
        "#     parser.add_argument('--test', action='store_true', default=False)\n",
        "#     parser.add_argument('--task', default='tsp')\n",
        "#     parser.add_argument('--nodes', dest='num_nodes', default=20, type=int)\n",
        "#     parser.add_argument('--actor_lr', default=5e-4, type=float)\n",
        "#     parser.add_argument('--critic_lr', default=5e-4, type=float)\n",
        "#     parser.add_argument('--max_grad_norm', default=2., type=float)\n",
        "#     parser.add_argument('--batch_size', default=256, type=int)\n",
        "#     parser.add_argument('--hidden', dest='hidden_size', default=128, type=int)\n",
        "#     parser.add_argument('--dropout', default=0.1, type=float)\n",
        "#     parser.add_argument('--layers', dest='num_layers', default=1, type=int)\n",
        "#     parser.add_argument('--train-size',default=1000000, type=int)\n",
        "#     parser.add_argument('--valid-size', default=1000, type=int)\n",
        "    \n",
        "    # Parameters \n",
        "    num_nodes = 20\n",
        "    train_size = 1000000\n",
        "    seed = 12345\n",
        "    valid_size = 1000\n",
        "    hidden_size = 128\n",
        "    num_layers = 100\n",
        "    checkpoint = None\n",
        "    dropout = 0.1\n",
        "    batch_size = 256\n",
        "    test = None #layers-nn\n",
        "    actor_lr = 5e-4\n",
        "    critic_lr = 5e-4\n",
        "    max_grad_norm = 2\n",
        "\n",
        "\n",
        "    train_data = TSPDataset(num_nodes, train_size, seed)\n",
        "    valid_data = TSPDataset(num_nodes, valid_size, seed + 1)\n",
        "\n",
        "    update_fn = None\n",
        "\n",
        "    actor = DRL4TSP(STATIC_SIZE,\n",
        "                    DYNAMIC_SIZE,\n",
        "                    hidden_size,\n",
        "                    update_fn,\n",
        "                    tsp.update_mask,\n",
        "                    num_layers,\n",
        "                    dropout).to(device)\n",
        "\n",
        "    critic = StateCritic(STATIC_SIZE, DYNAMIC_SIZE, hidden_size).to(device)\n",
        "\n",
        "    kwargs = {} #vars(args)\n",
        "    kwargs['train_data'] = train_data\n",
        "    kwargs['valid_data'] = valid_data\n",
        "    kwargs['reward_fn'] = tsp.reward\n",
        "    kwargs['render_fn'] = tsp.render\n",
        "\n",
        "    if checkpoint:\n",
        "        path = os.path.join(checkpoint, 'actor.pt')\n",
        "        actor.load_state_dict(torch.load(path, device))\n",
        "\n",
        "        path = os.path.join(checkpoint, 'critic.pt')\n",
        "        critic.load_state_dict(torch.load(path, device))\n",
        "    \n",
        "    if not test:\n",
        "        task = 'tsp'\n",
        "        train(actor, critic,task,num_nodes,train_data,valid_data, tsp.reward, tsp.render, batch_size,actor_lr, critic_lr, max_grad_norm ) #,kwargs\n",
        "\n",
        "    test_data = TSPDataset(num_nodes, train_size, seed + 2)\n",
        "\n",
        "    test_dir = 'test'\n",
        "    test_loader = DataLoader(test_data, batch_size, False, num_workers=0)\n",
        "    out = validate(test_loader, actor, tsp.reward, tsp.render, test_dir, num_plot=5)\n",
        "\n",
        "    print('Average tour length: ', out)\n",
        "\n",
        "\n",
        "# def train_vrp(args):\n",
        "\n",
        "#     # Goals from paper:\n",
        "#     # VRP10, Capacity 20:  4.84  (Greedy)\n",
        "#     # VRP20, Capacity 30:  6.59  (Greedy)\n",
        "#     # VRP50, Capacity 40:  11.39 (Greedy)\n",
        "#     # VRP100, Capacity 50: 17.23  (Greedy)\n",
        "\n",
        "#     # from RL_tsp_vrp.tasks import vrp\n",
        "#     # from tasks.vrp import VehicleRoutingDataset\n",
        "\n",
        "#     # Determines the maximum amount of load for a vehicle based on num nodes\n",
        "#     LOAD_DICT = {10: 20, 20: 30, 50: 40, 100: 50}\n",
        "#     MAX_DEMAND = 9\n",
        "#     STATIC_SIZE = 2 # (x, y)\n",
        "#     DYNAMIC_SIZE = 2 # (load, demand)\n",
        "\n",
        "#     max_load = LOAD_DICT[args.num_nodes]\n",
        "\n",
        "#     train_data = VehicleRoutingDataset(args.train_size,\n",
        "#                                        args.num_nodes,\n",
        "#                                        max_load,\n",
        "#                                        MAX_DEMAND,\n",
        "#                                        args.seed)\n",
        "\n",
        "#     valid_data = VehicleRoutingDataset(args.valid_size,\n",
        "#                                        args.num_nodes,\n",
        "#                                        max_load,\n",
        "#                                        MAX_DEMAND,\n",
        "#                                        args.seed + 1)\n",
        "#     # CHeck to device\n",
        "#     actor = DRL4TSP(STATIC_SIZE,\n",
        "#                     DYNAMIC_SIZE,\n",
        "#                     args.hidden_size,\n",
        "#                     train_data.update_dynamic,\n",
        "#                     train_data.update_mask,\n",
        "#                     args.num_layers,\n",
        "#                     args.dropout).to(device)\n",
        "\n",
        "#     critic = StateCritic(STATIC_SIZE, DYNAMIC_SIZE, args.hidden_size).to(device)\n",
        "\n",
        "#     kwargs = vars(args)\n",
        "#     kwargs['train_data'] = train_data\n",
        "#     kwargs['valid_data'] = valid_data\n",
        "#     kwargs['reward_fn'] = vrp.reward\n",
        "#     kwargs['render_fn'] = vrp.render\n",
        "\n",
        "#     if args.checkpoint:\n",
        "#         path = os.path.join(args.checkpoint, 'actor.pt')\n",
        "#         actor.load_state_dict(torch.load(path, device))\n",
        "\n",
        "#         path = os.path.join(args.checkpoint, 'critic.pt')\n",
        "#         critic.load_state_dict(torch.load(path, device))\n",
        "\n",
        "#     if not args.test:\n",
        "        \n",
        "#         train(actor, critic, **kwargs)\n",
        "\n",
        "#     test_data = VehicleRoutingDataset(args.valid_size,\n",
        "#                                       args.num_nodes,\n",
        "#                                       max_load,\n",
        "#                                       MAX_DEMAND,\n",
        "#                                       args.seed + 2)\n",
        "\n",
        "#     test_dir = 'test'\n",
        "#     test_loader = DataLoader(test_data, args.batch_size, False, num_workers=0)\n",
        "#     out = validate(test_loader, actor, vrp.reward, vrp.render, test_dir, num_plot=5)\n",
        "\n",
        "#     print('Average tour length: ', out)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "336c0c3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "336c0c3a",
        "outputId": "e6e9c695-06ad-472a-8aea-48ce8a926b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "  Batch 99/3907, reward: 9.781, loss: -137.4580, took: 63.6389s\n",
            "  Batch 199/3907, reward: 7.859, loss: -0.5299, took: 62.1159s\n",
            "  Batch 299/3907, reward: 7.615, loss: -0.4439, took: 61.5211s\n",
            "  Batch 399/3907, reward: 7.545, loss: -0.1097, took: 61.5433s\n",
            "  Batch 499/3907, reward: 7.523, loss: -0.2395, took: 62.0054s\n",
            "  Batch 599/3907, reward: 7.499, loss: -0.1323, took: 63.3036s\n",
            "  Batch 699/3907, reward: 7.489, loss: -0.1352, took: 63.3422s\n",
            "  Batch 799/3907, reward: 7.476, loss: -0.0247, took: 63.0544s\n",
            "  Batch 899/3907, reward: 7.468, loss: -0.1361, took: 63.2360s\n",
            "  Batch 999/3907, reward: 7.456, loss: -0.1439, took: 63.1774s\n",
            "  Batch 1099/3907, reward: 7.448, loss: -0.2112, took: 62.5061s\n",
            "  Batch 1199/3907, reward: 7.439, loss: -0.0779, took: 61.6308s\n",
            "  Batch 1299/3907, reward: 7.404, loss: -0.0479, took: 61.7293s\n",
            "  Batch 1399/3907, reward: 7.222, loss: -0.4644, took: 61.8613s\n",
            "  Batch 1499/3907, reward: 6.803, loss: -0.3871, took: 63.3152s\n",
            "  Batch 1599/3907, reward: 6.091, loss: -0.2497, took: 63.3862s\n",
            "  Batch 1699/3907, reward: 5.873, loss: -0.3426, took: 63.3321s\n",
            "  Batch 1799/3907, reward: 5.743, loss: -0.3555, took: 63.2105s\n",
            "  Batch 1899/3907, reward: 5.684, loss: -0.4291, took: 63.3545s\n",
            "  Batch 1999/3907, reward: 5.632, loss: -0.3198, took: 62.6737s\n",
            "  Batch 2099/3907, reward: 5.561, loss: -0.3625, took: 61.7589s\n",
            "  Batch 2199/3907, reward: 5.532, loss: -0.3387, took: 61.5507s\n",
            "  Batch 2299/3907, reward: 5.503, loss: -0.3360, took: 61.5336s\n",
            "  Batch 2399/3907, reward: 5.454, loss: -0.3061, took: 62.5508s\n",
            "  Batch 2499/3907, reward: 5.420, loss: -0.2993, took: 63.3567s\n",
            "  Batch 2599/3907, reward: 5.357, loss: -0.3564, took: 63.1934s\n",
            "  Batch 2699/3907, reward: 5.320, loss: -0.3461, took: 63.2616s\n",
            "  Batch 2799/3907, reward: 5.290, loss: -0.3672, took: 63.1373s\n",
            "  Batch 2899/3907, reward: 5.274, loss: -0.2867, took: 63.3124s\n",
            "  Batch 2999/3907, reward: 5.239, loss: -0.1875, took: 61.9076s\n",
            "  Batch 3099/3907, reward: 5.219, loss: -0.2781, took: 61.5388s\n",
            "  Batch 3199/3907, reward: 5.213, loss: -0.2450, took: 61.6624s\n",
            "  Batch 3299/3907, reward: 5.188, loss: -0.2220, took: 61.4558s\n",
            "  Batch 3399/3907, reward: 5.176, loss: -0.3031, took: 63.0619s\n",
            "  Batch 3499/3907, reward: 5.159, loss: -0.3115, took: 63.1132s\n",
            "  Batch 3599/3907, reward: 5.152, loss: -0.2337, took: 63.2002s\n",
            "  Batch 3699/3907, reward: 5.140, loss: -0.2757, took: 63.0169s\n",
            "  Batch 3799/3907, reward: 5.137, loss: -0.1997, took: 63.0745s\n",
            "  Batch 3899/3907, reward: 5.112, loss: -0.2014, took: 62.6156s\n",
            "Mean epoch loss/reward: -3.7801, 6.2414, 5.0027, took: 2454.8861s (62.6215s / 100 batches)\n",
            "\n",
            "1\n",
            "  Batch 99/3907, reward: 5.109, loss: -0.2183, took: 61.6495s\n",
            "  Batch 199/3907, reward: 5.119, loss: -0.2057, took: 61.5441s\n",
            "  Batch 299/3907, reward: 5.087, loss: -0.1903, took: 61.6395s\n",
            "  Batch 399/3907, reward: 5.082, loss: -0.2345, took: 62.6320s\n",
            "  Batch 499/3907, reward: 5.071, loss: -0.2613, took: 63.2443s\n",
            "  Batch 599/3907, reward: 5.070, loss: -0.2404, took: 63.2244s\n",
            "  Batch 699/3907, reward: 5.055, loss: -0.1981, took: 63.2048s\n",
            "  Batch 799/3907, reward: 5.067, loss: -0.1768, took: 63.3244s\n",
            "  Batch 899/3907, reward: 5.056, loss: -0.1882, took: 63.2575s\n",
            "  Batch 999/3907, reward: 5.058, loss: -0.2143, took: 61.6576s\n",
            "  Batch 1099/3907, reward: 5.031, loss: -0.1937, took: 61.7609s\n",
            "  Batch 1199/3907, reward: 5.022, loss: -0.2166, took: 61.6099s\n",
            "  Batch 1299/3907, reward: 5.017, loss: -0.2070, took: 61.5426s\n",
            "  Batch 1399/3907, reward: 5.009, loss: -0.1796, took: 62.4401s\n",
            "  Batch 1499/3907, reward: 5.004, loss: -0.1742, took: 63.3121s\n",
            "  Batch 1599/3907, reward: 5.005, loss: -0.1371, took: 63.2659s\n",
            "  Batch 1699/3907, reward: 5.017, loss: -0.1882, took: 63.1973s\n",
            "  Batch 1799/3907, reward: 4.986, loss: -0.1550, took: 63.2043s\n",
            "  Batch 1899/3907, reward: 4.993, loss: -0.1344, took: 63.3421s\n",
            "  Batch 1999/3907, reward: 4.973, loss: -0.1870, took: 61.9929s\n",
            "  Batch 2099/3907, reward: 4.976, loss: -0.1806, took: 61.6512s\n",
            "  Batch 2199/3907, reward: 4.974, loss: -0.2039, took: 61.5347s\n",
            "  Batch 2299/3907, reward: 4.967, loss: -0.1851, took: 61.5321s\n",
            "  Batch 2399/3907, reward: 4.960, loss: -0.1521, took: 61.7279s\n",
            "  Batch 2499/3907, reward: 4.959, loss: -0.1406, took: 63.2024s\n",
            "  Batch 2599/3907, reward: 4.966, loss: -0.1293, took: 63.2080s\n",
            "  Batch 2699/3907, reward: 4.943, loss: -0.1721, took: 63.1560s\n",
            "  Batch 2799/3907, reward: 4.942, loss: -0.2490, took: 63.1079s\n",
            "  Batch 2899/3907, reward: 4.938, loss: -0.1537, took: 63.1199s\n",
            "  Batch 2999/3907, reward: 4.935, loss: -0.1651, took: 62.4987s\n",
            "  Batch 3099/3907, reward: 4.924, loss: -0.1922, took: 61.5011s\n",
            "  Batch 3199/3907, reward: 4.922, loss: -0.1776, took: 61.5135s\n",
            "  Batch 3299/3907, reward: 4.915, loss: -0.2140, took: 61.4663s\n",
            "  Batch 3399/3907, reward: 4.919, loss: -0.1806, took: 61.4134s\n",
            "  Batch 3499/3907, reward: 4.905, loss: -0.1545, took: 62.5679s\n",
            "  Batch 3599/3907, reward: 4.901, loss: -0.1411, took: 63.0799s\n",
            "  Batch 3699/3907, reward: 4.911, loss: -0.1334, took: 63.0933s\n",
            "  Batch 3799/3907, reward: 4.931, loss: -0.1825, took: 63.0808s\n",
            "  Batch 3899/3907, reward: 4.889, loss: -0.1710, took: 63.1382s\n",
            "Mean epoch loss/reward: -0.1840, 4.9898, 4.8035, took: 2449.3799s (62.4779s / 100 batches)\n",
            "\n",
            "2\n",
            "  Batch 99/3907, reward: 4.885, loss: -0.1540, took: 62.7094s\n",
            "  Batch 199/3907, reward: 4.886, loss: -0.1927, took: 61.4233s\n",
            "  Batch 299/3907, reward: 4.879, loss: -0.1864, took: 61.3722s\n",
            "  Batch 399/3907, reward: 4.892, loss: -0.1745, took: 61.4515s\n",
            "  Batch 499/3907, reward: 4.881, loss: -0.1387, took: 61.3458s\n",
            "  Batch 599/3907, reward: 4.878, loss: -0.1577, took: 62.6591s\n",
            "  Batch 699/3907, reward: 4.862, loss: -0.1596, took: 63.2300s\n",
            "  Batch 799/3907, reward: 4.870, loss: -0.1813, took: 63.0303s\n",
            "  Batch 899/3907, reward: 4.872, loss: -0.1023, took: 63.0977s\n",
            "  Batch 999/3907, reward: 4.866, loss: -0.1869, took: 63.1752s\n",
            "  Batch 1099/3907, reward: 4.863, loss: -0.1551, took: 63.0823s\n",
            "  Batch 1199/3907, reward: 4.868, loss: -0.1651, took: 61.5845s\n",
            "  Batch 1299/3907, reward: 4.866, loss: -0.1610, took: 61.5428s\n",
            "  Batch 1399/3907, reward: 4.859, loss: -0.1698, took: 61.7006s\n",
            "  Batch 1499/3907, reward: 4.872, loss: -0.2406, took: 61.9125s\n",
            "  Batch 1599/3907, reward: 4.845, loss: -0.1553, took: 63.1968s\n",
            "  Batch 1699/3907, reward: 4.851, loss: -0.1564, took: 63.2971s\n",
            "  Batch 1799/3907, reward: 4.844, loss: -0.2124, took: 63.3533s\n",
            "  Batch 1899/3907, reward: 4.830, loss: -0.1161, took: 63.4280s\n",
            "  Batch 1999/3907, reward: 4.831, loss: -0.1397, took: 63.3576s\n",
            "  Batch 2099/3907, reward: 4.827, loss: -0.1501, took: 62.6603s\n",
            "  Batch 2199/3907, reward: 4.829, loss: -0.1459, took: 61.6643s\n",
            "  Batch 2299/3907, reward: 4.819, loss: -0.0859, took: 61.8499s\n",
            "  Batch 2399/3907, reward: 4.828, loss: -0.1369, took: 61.7185s\n",
            "  Batch 2499/3907, reward: 4.819, loss: -0.1776, took: 61.7890s\n",
            "  Batch 2599/3907, reward: 4.816, loss: -0.1159, took: 63.1022s\n",
            "  Batch 2699/3907, reward: 4.810, loss: -0.1719, took: 63.0470s\n",
            "  Batch 2799/3907, reward: 4.807, loss: -0.1599, took: 63.2868s\n",
            "  Batch 2899/3907, reward: 4.809, loss: -0.1427, took: 63.1791s\n",
            "  Batch 2999/3907, reward: 4.814, loss: -0.1421, took: 63.2818s\n",
            "  Batch 3099/3907, reward: 4.807, loss: -0.1618, took: 62.4565s\n",
            "  Batch 3199/3907, reward: 4.802, loss: -0.1781, took: 61.7838s\n",
            "  Batch 3299/3907, reward: 4.800, loss: -0.1730, took: 61.7093s\n",
            "  Batch 3399/3907, reward: 4.795, loss: -0.1525, took: 61.5529s\n",
            "  Batch 3499/3907, reward: 4.792, loss: -0.1557, took: 62.2323s\n",
            "  Batch 3599/3907, reward: 4.821, loss: -0.1443, took: 63.2373s\n",
            "  Batch 3699/3907, reward: 4.801, loss: -0.1496, took: 63.1570s\n",
            "  Batch 3799/3907, reward: 4.793, loss: -0.1270, took: 63.1917s\n",
            "  Batch 3899/3907, reward: 4.790, loss: -0.1491, took: 63.1799s\n",
            "Mean epoch loss/reward: -0.1566, 4.8378, 4.7261, took: 2450.6986s (62.5136s / 100 batches)\n",
            "\n",
            "3\n",
            "  Batch 99/3907, reward: 4.788, loss: -0.1513, took: 63.2648s\n",
            "  Batch 199/3907, reward: 4.793, loss: -0.1419, took: 61.6111s\n",
            "  Batch 299/3907, reward: 4.780, loss: -0.1617, took: 61.4856s\n",
            "  Batch 399/3907, reward: 4.774, loss: -0.1196, took: 61.3729s\n",
            "  Batch 499/3907, reward: 4.776, loss: -0.1106, took: 61.5369s\n",
            "  Batch 599/3907, reward: 4.779, loss: -0.1418, took: 61.3119s\n",
            "  Batch 699/3907, reward: 4.783, loss: -0.1430, took: 62.3623s\n",
            "  Batch 799/3907, reward: 4.778, loss: -0.1333, took: 63.1387s\n",
            "  Batch 899/3907, reward: 4.780, loss: -0.1100, took: 63.1709s\n",
            "  Batch 999/3907, reward: 4.784, loss: -0.1415, took: 63.1529s\n",
            "  Batch 1099/3907, reward: 4.777, loss: -0.1625, took: 63.1459s\n",
            "  Batch 1199/3907, reward: 4.774, loss: -0.1194, took: 63.2012s\n",
            "  Batch 1299/3907, reward: 4.769, loss: -0.1228, took: 61.6379s\n",
            "  Batch 1399/3907, reward: 4.768, loss: -0.1073, took: 61.3456s\n",
            "  Batch 1499/3907, reward: 4.772, loss: -0.1321, took: 61.4770s\n",
            "  Batch 1599/3907, reward: 4.769, loss: -0.1057, took: 61.4401s\n",
            "  Batch 1699/3907, reward: 4.768, loss: -0.1448, took: 61.4303s\n",
            "  Batch 1799/3907, reward: 4.767, loss: -0.1468, took: 62.3298s\n",
            "  Batch 1899/3907, reward: 4.766, loss: -0.1479, took: 63.1538s\n",
            "  Batch 1999/3907, reward: 4.758, loss: -0.0871, took: 63.2138s\n",
            "  Batch 2099/3907, reward: 4.766, loss: -0.1137, took: 63.1321s\n",
            "  Batch 2199/3907, reward: 4.757, loss: -0.1144, took: 63.2659s\n",
            "  Batch 2299/3907, reward: 4.756, loss: -0.1392, took: 63.4111s\n",
            "  Batch 2399/3907, reward: 4.765, loss: -0.1278, took: 61.7148s\n",
            "  Batch 2499/3907, reward: 4.758, loss: -0.1259, took: 61.4847s\n",
            "  Batch 2599/3907, reward: 4.752, loss: -0.1046, took: 61.9301s\n",
            "  Batch 2699/3907, reward: 4.759, loss: -0.1299, took: 61.4131s\n",
            "  Batch 2799/3907, reward: 4.755, loss: -0.1179, took: 61.5587s\n",
            "  Batch 2899/3907, reward: 4.760, loss: -0.1201, took: 62.4787s\n",
            "  Batch 2999/3907, reward: 4.747, loss: -0.1216, took: 63.4433s\n",
            "  Batch 3099/3907, reward: 4.751, loss: -0.1356, took: 63.3372s\n",
            "  Batch 3199/3907, reward: 4.744, loss: -0.0835, took: 63.2968s\n",
            "  Batch 3299/3907, reward: 4.751, loss: -0.1208, took: 63.4693s\n",
            "  Batch 3399/3907, reward: 4.747, loss: -0.1303, took: 62.9992s\n",
            "  Batch 3499/3907, reward: 4.745, loss: -0.1098, took: 61.4101s\n",
            "  Batch 3599/3907, reward: 4.741, loss: -0.1178, took: 61.3798s\n",
            "  Batch 3699/3907, reward: 4.747, loss: -0.1244, took: 61.3390s\n",
            "  Batch 3799/3907, reward: 4.738, loss: -0.0897, took: 61.2493s\n",
            "  Batch 3899/3907, reward: 4.742, loss: -0.1081, took: 61.4092s\n",
            "Mean epoch loss/reward: -0.1248, 4.7636, 4.6852, took: 2441.0411s (62.2694s / 100 batches)\n",
            "\n",
            "4\n",
            "  Batch 99/3907, reward: 4.731, loss: -0.1247, took: 63.1150s\n",
            "  Batch 199/3907, reward: 4.738, loss: -0.1301, took: 63.1707s\n",
            "  Batch 299/3907, reward: 4.730, loss: -0.1057, took: 63.1339s\n",
            "  Batch 399/3907, reward: 4.736, loss: -0.1308, took: 63.2000s\n",
            "  Batch 499/3907, reward: 4.735, loss: -0.1004, took: 63.1281s\n",
            "  Batch 599/3907, reward: 4.736, loss: -0.1330, took: 62.7073s\n",
            "  Batch 699/3907, reward: 4.731, loss: -0.1404, took: 61.5263s\n",
            "  Batch 799/3907, reward: 4.737, loss: -0.1087, took: 61.6040s\n",
            "  Batch 899/3907, reward: 4.736, loss: -0.1446, took: 61.4056s\n",
            "  Batch 999/3907, reward: 4.730, loss: -0.0980, took: 61.3540s\n",
            "  Batch 1099/3907, reward: 4.727, loss: -0.1095, took: 63.1749s\n",
            "  Batch 1199/3907, reward: 4.725, loss: -0.1200, took: 63.1835s\n",
            "  Batch 1299/3907, reward: 4.732, loss: -0.1147, took: 63.1225s\n",
            "  Batch 1399/3907, reward: 4.729, loss: -0.1108, took: 63.3161s\n",
            "  Batch 1499/3907, reward: 4.729, loss: -0.1240, took: 63.1991s\n",
            "  Batch 1599/3907, reward: 4.722, loss: -0.1113, took: 62.4836s\n",
            "  Batch 1699/3907, reward: 4.719, loss: -0.1115, took: 61.6293s\n",
            "  Batch 1799/3907, reward: 4.718, loss: -0.1054, took: 61.4358s\n",
            "  Batch 1899/3907, reward: 4.715, loss: -0.0888, took: 61.4970s\n",
            "  Batch 1999/3907, reward: 4.714, loss: -0.1223, took: 61.2984s\n",
            "  Batch 2099/3907, reward: 4.718, loss: -0.0940, took: 61.3640s\n",
            "  Batch 2199/3907, reward: 4.720, loss: -0.1026, took: 62.5914s\n",
            "  Batch 2299/3907, reward: 4.724, loss: -0.1139, took: 63.2269s\n",
            "  Batch 2399/3907, reward: 4.723, loss: -0.1290, took: 63.2251s\n",
            "  Batch 2499/3907, reward: 4.718, loss: -0.0980, took: 63.0845s\n",
            "  Batch 2599/3907, reward: 4.715, loss: -0.1117, took: 63.1074s\n",
            "  Batch 2699/3907, reward: 4.720, loss: -0.1213, took: 63.3061s\n",
            "  Batch 2799/3907, reward: 4.717, loss: -0.1233, took: 61.7336s\n",
            "  Batch 2899/3907, reward: 4.717, loss: -0.1166, took: 61.5226s\n",
            "  Batch 2999/3907, reward: 4.713, loss: -0.1051, took: 61.4975s\n",
            "  Batch 3099/3907, reward: 4.708, loss: -0.1207, took: 61.2718s\n",
            "  Batch 3199/3907, reward: 4.707, loss: -0.0832, took: 61.3973s\n",
            "  Batch 3299/3907, reward: 4.716, loss: -0.1022, took: 61.6749s\n",
            "  Batch 3399/3907, reward: 4.717, loss: -0.1651, took: 63.2990s\n",
            "  Batch 3499/3907, reward: 4.705, loss: -0.0923, took: 63.1716s\n",
            "  Batch 3599/3907, reward: 4.711, loss: -0.1136, took: 63.5033s\n",
            "  Batch 3699/3907, reward: 4.732, loss: -0.1127, took: 63.3574s\n",
            "  Batch 3799/3907, reward: 4.713, loss: -0.1110, took: 63.1226s\n",
            "  Batch 3899/3907, reward: 4.709, loss: -0.0914, took: 62.4506s\n",
            "Mean epoch loss/reward: -0.1142, 4.7224, 4.6529, took: 2449.0658s (62.4767s / 100 batches)\n",
            "\n",
            "5\n",
            "  Batch 99/3907, reward: 4.709, loss: -0.1254, took: 61.6310s\n",
            "  Batch 199/3907, reward: 4.705, loss: -0.0819, took: 61.4617s\n",
            "  Batch 299/3907, reward: 4.704, loss: -0.1209, took: 61.5295s\n",
            "  Batch 399/3907, reward: 4.701, loss: -0.0820, took: 61.5313s\n",
            "  Batch 499/3907, reward: 4.706, loss: -0.1407, took: 61.7141s\n",
            "  Batch 599/3907, reward: 4.704, loss: -0.0840, took: 63.3661s\n",
            "  Batch 699/3907, reward: 4.703, loss: -0.1208, took: 63.3057s\n",
            "  Batch 799/3907, reward: 4.708, loss: -0.1000, took: 63.2552s\n",
            "  Batch 899/3907, reward: 4.704, loss: -0.1282, took: 63.1614s\n",
            "  Batch 999/3907, reward: 4.699, loss: -0.1078, took: 63.2701s\n",
            "  Batch 1099/3907, reward: 4.702, loss: -0.1114, took: 62.5070s\n",
            "  Batch 1199/3907, reward: 4.702, loss: -0.0962, took: 61.6819s\n",
            "  Batch 1299/3907, reward: 4.698, loss: -0.1155, took: 61.5152s\n",
            "  Batch 1399/3907, reward: 4.703, loss: -0.0919, took: 61.5315s\n",
            "  Batch 1499/3907, reward: 4.696, loss: -0.1262, took: 61.7908s\n",
            "  Batch 1599/3907, reward: 4.705, loss: -0.1039, took: 61.5528s\n",
            "  Batch 1699/3907, reward: 4.707, loss: -0.1068, took: 62.9349s\n",
            "  Batch 1799/3907, reward: 4.696, loss: -0.0794, took: 63.1703s\n",
            "  Batch 1899/3907, reward: 4.693, loss: -0.0873, took: 63.3119s\n",
            "  Batch 1999/3907, reward: 4.693, loss: -0.0853, took: 63.2590s\n",
            "  Batch 2099/3907, reward: 4.697, loss: -0.0840, took: 63.2697s\n",
            "  Batch 2199/3907, reward: 4.697, loss: -0.1014, took: 63.0051s\n",
            "  Batch 2299/3907, reward: 4.693, loss: -0.0965, took: 61.5284s\n",
            "  Batch 2399/3907, reward: 4.696, loss: -0.1317, took: 61.5381s\n",
            "  Batch 2499/3907, reward: 4.692, loss: -0.0851, took: 61.3662s\n",
            "  Batch 2599/3907, reward: 4.701, loss: -0.1062, took: 61.4527s\n",
            "  Batch 2699/3907, reward: 4.709, loss: -0.0806, took: 61.2088s\n",
            "  Batch 2799/3907, reward: 4.696, loss: -0.0971, took: 61.4217s\n",
            "  Batch 2899/3907, reward: 4.693, loss: -0.1175, took: 61.5606s\n",
            "  Batch 2999/3907, reward: 4.691, loss: -0.0893, took: 63.2178s\n",
            "  Batch 3099/3907, reward: 4.690, loss: -0.1169, took: 63.2383s\n",
            "  Batch 3199/3907, reward: 4.691, loss: -0.0762, took: 63.3968s\n",
            "  Batch 3299/3907, reward: 4.689, loss: -0.1285, took: 63.2957s\n",
            "  Batch 3399/3907, reward: 4.693, loss: -0.0948, took: 63.4622s\n",
            "  Batch 3499/3907, reward: 4.692, loss: -0.0896, took: 62.4793s\n",
            "  Batch 3599/3907, reward: 4.693, loss: -0.1217, took: 61.5627s\n",
            "  Batch 3699/3907, reward: 4.689, loss: -0.0720, took: 61.5131s\n",
            "  Batch 3799/3907, reward: 4.691, loss: -0.0888, took: 61.5669s\n",
            "  Batch 3899/3907, reward: 4.700, loss: -0.1099, took: 61.5381s\n",
            "Mean epoch loss/reward: -0.1021, 4.6982, 4.6656, took: 2441.5158s (62.2847s / 100 batches)\n",
            "\n",
            "Average tour length:  4.669975217419388\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "#     parser = argparse.ArgumentParser(description='Combinatorial Optimization')\n",
        "#     parser.add_argument('--seed', default=12345, type=int)\n",
        "#     parser.add_argument('--checkpoint', default=None)\n",
        "#     parser.add_argument('--test', action='store_true', default=False)\n",
        "#     parser.add_argument('--task', default='tsp')\n",
        "#     parser.add_argument('--nodes', dest='num_nodes', default=20, type=int)\n",
        "#     parser.add_argument('--actor_lr', default=5e-4, type=float)\n",
        "#     parser.add_argument('--critic_lr', default=5e-4, type=float)\n",
        "#     parser.add_argument('--max_grad_norm', default=2., type=float)\n",
        "#     parser.add_argument('--batch_size', default=256, type=int)\n",
        "#     parser.add_argument('--hidden', dest='hidden_size', default=128, type=int)\n",
        "#     parser.add_argument('--dropout', default=0.1, type=float)\n",
        "#     parser.add_argument('--layers', dest='num_layers', default=1, type=int)\n",
        "#     parser.add_argument('--train-size',default=1000000, type=int)\n",
        "#     parser.add_argument('--valid-size', default=1000, type=int)\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     print('NOTE: SETTTING CHECKPOINT: ')\n",
        "#     args.checkpoint = os.path.join('vrp', '10', '12_59_47.350165' + os.path.sep)\n",
        "#     print(args.checkpoint)\n",
        "\n",
        "    task = 'tsp' \n",
        "    \n",
        "    if task == 'tsp':\n",
        "        train_tsp()\n",
        "    elif args.task == 'vrp':\n",
        "        train_vrp(args)\n",
        "    else:\n",
        "        raise ValueError('Task <%s> not understood'%args.task)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3ff1db71",
      "metadata": {
        "id": "3ff1db71"
      },
      "outputs": [],
      "source": [
        "# parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8d8ef7cd",
      "metadata": {
        "id": "8d8ef7cd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "optim_rl_colab.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}