{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pr33tham7/RL_tsp_vrp/blob/main/optim_rl_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6475985d",
      "metadata": {
        "id": "6475985d"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pr33tham7/RL_tsp_vrp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNKAwo2VGZdY",
        "outputId": "e2fed679-2b5a-407f-a96b-34ba3b4e324e"
      },
      "id": "CNKAwo2VGZdY",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RL_tsp_vrp'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 44 (delta 7), reused 31 (delta 0), pack-reused 0\n",
            "Unpacking objects: 100% (44/44), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "09c98831",
      "metadata": {
        "id": "09c98831"
      },
      "outputs": [],
      "source": [
        "\"\"\"Defines the main trainer model for combinatorial problems\n",
        "\n",
        "Each task must define the following functions:\n",
        "* mask_fn: can be None\n",
        "* update_fn: can be None\n",
        "* reward_fn: specifies the quality of found solutions\n",
        "* render_fn: Specifies how to plot found solutions. Can be None\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from RL_tsp_vrp import *\n",
        "from RL_tsp_vrp.model import DRL4TSP, Encoder\n",
        "from RL_tsp_vrp.tasks import tsp\n",
        "from RL_tsp_vrp.tasks.tsp import TSPDataset\n",
        "from RL_tsp_vrp.tasks import vrp\n",
        "from RL_tsp_vrp.tasks.vrp import VehicleRoutingDataset\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Critic network that estimates the reward for any problem instance from a given state\n",
        "class StateCritic(nn.Module):\n",
        "\n",
        "    def __init__(self, static_size, dynamic_size, hidden_size):\n",
        "        super(StateCritic, self).__init__()\n",
        "        \n",
        "        # Static state input to the Encoder\n",
        "        self.static_encoder = Encoder(static_size, hidden_size)\n",
        "\n",
        "        # Dynamic state input to the Encoder\n",
        "        self.dynamic_encoder = Encoder(dynamic_size, hidden_size)\n",
        "\n",
        "        # Define the encoder & decoder models\n",
        "        # Network architecture\n",
        "        self.fc1 = nn.Conv1d(hidden_size * 2, 20, kernel_size=1)\n",
        "        self.fc2 = nn.Conv1d(20, 20, kernel_size=1)\n",
        "        self.fc3 = nn.Conv1d(20, 1, kernel_size=1)\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if len(p.shape) > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, static, dynamic):\n",
        "\n",
        "        # Use the probabilities of visiting each\n",
        "        static_hidden = self.static_encoder(static)\n",
        "        dynamic_hidden = self.dynamic_encoder(dynamic)\n",
        "\n",
        "        hidden = torch.cat((static_hidden, dynamic_hidden), 1)\n",
        "\n",
        "        output = F.relu(self.fc1(hidden))\n",
        "        output = F.relu(self.fc2(output))\n",
        "        output = self.fc3(output).sum(dim=2)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Function to evaluate the validation set from the locations database\n",
        "def validate(data_loader, actor, reward_fn, render_fn=None, save_dir='.',\n",
        "             num_plot=5):\n",
        "    \n",
        "    # Evaluate the actor network that predicts a probability distribution over the next action at any given decision step\n",
        "    actor.eval()\n",
        "\n",
        "    # Create a directory to sabe the plots of the paths generated for the trips evaluated\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    # Evaluate for each batch of validation dataset\n",
        "    rewards = []\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        static, dynamic, x0 = batch\n",
        "\n",
        "        static = static.to(device)\n",
        "        dynamic = dynamic.to(device)\n",
        "        x0 = x0.to(device) if len(x0) > 0 else None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            tour_indices, _ = actor.forward(static, dynamic, x0)\n",
        "        \n",
        "        # Calculate the reward and append it to rewards history\n",
        "        reward = reward_fn(static, tour_indices).mean().item()\n",
        "        rewards.append(reward)\n",
        "\n",
        "        if render_fn is not None and batch_idx < num_plot:\n",
        "            name = 'batch%d_%2.4f.png'%(batch_idx, reward)\n",
        "            path = os.path.join(save_dir, name)\n",
        "            render_fn(static, tour_indices, path)\n",
        "\n",
        "    actor.train()\n",
        "    return np.mean(rewards)\n",
        "\n",
        "\n",
        "def train(actor, critic, task, num_nodes, train_data, valid_data, reward_fn,\n",
        "          render_fn, batch_size, actor_lr, critic_lr, max_grad_norm,\n",
        "          **kwargs):\n",
        "    \"\"\"Constructs the main actor & critic networks, and performs all training.\"\"\"\n",
        "\n",
        "    now = '%s' % datetime.datetime.now().time()\n",
        "    now = now.replace(':', '_')\n",
        "    save_dir = os.path.join(task, '%d' % num_nodes, now)\n",
        "\n",
        "    checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    actor_optim = optim.Adam(actor.parameters(), lr=actor_lr)\n",
        "    critic_optim = optim.Adam(critic.parameters(), lr=critic_lr)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size, True, num_workers=0)\n",
        "    valid_loader = DataLoader(valid_data, batch_size, False, num_workers=0)\n",
        "\n",
        "    best_params = None\n",
        "    best_reward = np.inf\n",
        "\n",
        "    for epoch in range(6):\n",
        "        print(epoch)\n",
        "        actor.train()\n",
        "        critic.train()\n",
        "\n",
        "        times, losses, rewards, critic_rewards = [], [], [], []\n",
        "\n",
        "        epoch_start = time.time()\n",
        "        start = epoch_start\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "            static, dynamic, x0 = batch\n",
        "\n",
        "            static = static.to(device)\n",
        "            dynamic = dynamic.to(device)\n",
        "            x0 = x0.to(device) if len(x0) > 0 else None\n",
        "\n",
        "            # Full forward pass through the dataset\n",
        "            tour_indices, tour_logp = actor(static, dynamic, x0)\n",
        "\n",
        "            # Sum the log probabilities for each city in the tour\n",
        "            reward = reward_fn(static, tour_indices)\n",
        "\n",
        "            # Query the critic for an estimate of the reward\n",
        "            critic_est = critic(static, dynamic).view(-1)\n",
        "\n",
        "            advantage = (reward - critic_est)\n",
        "            actor_loss = torch.mean(advantage.detach() * tour_logp.sum(dim=1))\n",
        "            critic_loss = torch.mean(advantage ** 2)\n",
        "\n",
        "            actor_optim.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(actor.parameters(), max_grad_norm)\n",
        "            actor_optim.step()\n",
        "\n",
        "            critic_optim.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(critic.parameters(), max_grad_norm)\n",
        "            critic_optim.step()\n",
        "\n",
        "            critic_rewards.append(torch.mean(critic_est.detach()).item())\n",
        "            rewards.append(torch.mean(reward.detach()).item())\n",
        "            losses.append(torch.mean(actor_loss.detach()).item())\n",
        "\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                end = time.time()\n",
        "                times.append(end - start)\n",
        "                start = end\n",
        "\n",
        "                mean_loss = np.mean(losses[-100:])\n",
        "                mean_reward = np.mean(rewards[-100:])\n",
        "\n",
        "                print('  Batch %d/%d, reward: %2.3f, loss: %2.4f, took: %2.4fs' %\n",
        "                      (batch_idx, len(train_loader), mean_reward, mean_loss,\n",
        "                       times[-1]))\n",
        "\n",
        "        mean_loss = np.mean(losses)\n",
        "        mean_reward = np.mean(rewards)\n",
        "\n",
        "        # Save the weights\n",
        "        epoch_dir = os.path.join(checkpoint_dir, '%s' % epoch)\n",
        "        if not os.path.exists(epoch_dir):\n",
        "            os.makedirs(epoch_dir)\n",
        "\n",
        "        save_path = os.path.join(epoch_dir, 'actor.pt')\n",
        "        torch.save(actor.state_dict(), save_path)\n",
        "\n",
        "        save_path = os.path.join(epoch_dir, 'critic.pt')\n",
        "        torch.save(critic.state_dict(), save_path)\n",
        "\n",
        "        # Save rendering of validation set tours\n",
        "        valid_dir = os.path.join(save_dir, '%s' % epoch)\n",
        "\n",
        "        mean_valid = validate(valid_loader, actor, reward_fn, render_fn,\n",
        "                              valid_dir, num_plot=5)\n",
        "\n",
        "        # Save best model parameters\n",
        "        if mean_valid < best_reward:\n",
        "\n",
        "            best_reward = mean_valid\n",
        "\n",
        "            save_path = os.path.join(save_dir, 'actor.pt')\n",
        "            torch.save(actor.state_dict(), save_path)\n",
        "\n",
        "            save_path = os.path.join(save_dir, 'critic.pt')\n",
        "            torch.save(critic.state_dict(), save_path)\n",
        "\n",
        "        print('Mean epoch loss/reward: %2.4f, %2.4f, %2.4f, took: %2.4fs '\\\n",
        "              '(%2.4fs / 100 batches)\\n' % \\\n",
        "              (mean_loss, mean_reward, mean_valid, time.time() - epoch_start,\n",
        "              np.mean(times)))\n",
        "\n",
        "\n",
        "\n",
        "def train_tsp():#args\n",
        "\n",
        "    # Goals from paper:\n",
        "    # TSP20, 3.97\n",
        "    # TSP50, 6.08\n",
        "    # TSP100, 8.44\n",
        "\n",
        "    # from RL_tsp_vrp.tasks import tsp\n",
        "    # from tasks.tsp import TSPDataset\n",
        "\n",
        "    STATIC_SIZE = 2 # (x, y)\n",
        "    DYNAMIC_SIZE = 1 # dummy for compatibility\n",
        "    \n",
        "    \n",
        "#  parser = argparse.ArgumentParser(description='Combinatorial Optimization')\n",
        "#     parser.add_argument('--seed', default=12345, type=int)\n",
        "#     parser.add_argument('--checkpoint', default=None)\n",
        "#     parser.add_argument('--test', action='store_true', default=False)\n",
        "#     parser.add_argument('--task', default='tsp')\n",
        "#     parser.add_argument('--nodes', dest='num_nodes', default=20, type=int)\n",
        "#     parser.add_argument('--actor_lr', default=5e-4, type=float)\n",
        "#     parser.add_argument('--critic_lr', default=5e-4, type=float)\n",
        "#     parser.add_argument('--max_grad_norm', default=2., type=float)\n",
        "#     parser.add_argument('--batch_size', default=256, type=int)\n",
        "#     parser.add_argument('--hidden', dest='hidden_size', default=128, type=int)\n",
        "#     parser.add_argument('--dropout', default=0.1, type=float)\n",
        "#     parser.add_argument('--layers', dest='num_layers', default=1, type=int)\n",
        "#     parser.add_argument('--train-size',default=1000000, type=int)\n",
        "#     parser.add_argument('--valid-size', default=1000, type=int)\n",
        "    \n",
        "    # Parameters \n",
        "    num_nodes = 20\n",
        "    train_size = 1000000\n",
        "    seed = 12345\n",
        "    valid_size = 1000\n",
        "    hidden_size = 128\n",
        "    num_layers = 100\n",
        "    checkpoint = None\n",
        "    dropout = 0.1\n",
        "    batch_size = 256\n",
        "    test = None #layers-nn\n",
        "    actor_lr = 5e-4\n",
        "    critic_lr = 5e-4\n",
        "    max_grad_norm = 2\n",
        "\n",
        "\n",
        "    train_data = TSPDataset(num_nodes, train_size, seed)\n",
        "    valid_data = TSPDataset(num_nodes, valid_size, seed + 1)\n",
        "\n",
        "    update_fn = None\n",
        "\n",
        "    actor = DRL4TSP(STATIC_SIZE,\n",
        "                    DYNAMIC_SIZE,\n",
        "                    hidden_size,\n",
        "                    update_fn,\n",
        "                    tsp.update_mask,\n",
        "                    num_layers,\n",
        "                    dropout).to(device)\n",
        "\n",
        "    critic = StateCritic(STATIC_SIZE, DYNAMIC_SIZE, hidden_size).to(device)\n",
        "\n",
        "    kwargs = {} #vars(args)\n",
        "    kwargs['train_data'] = train_data\n",
        "    kwargs['valid_data'] = valid_data\n",
        "    kwargs['reward_fn'] = tsp.reward\n",
        "    kwargs['render_fn'] = tsp.render\n",
        "\n",
        "    if checkpoint:\n",
        "        path = os.path.join(checkpoint, 'actor.pt')\n",
        "        actor.load_state_dict(torch.load(path, device))\n",
        "\n",
        "        path = os.path.join(checkpoint, 'critic.pt')\n",
        "        critic.load_state_dict(torch.load(path, device))\n",
        "    \n",
        "    if not test:\n",
        "        task = 'tsp'\n",
        "        train(actor, critic,task,num_nodes,train_data,valid_data, tsp.reward, tsp.render, batch_size,actor_lr, critic_lr, max_grad_norm ) #,kwargs\n",
        "\n",
        "    test_data = TSPDataset(num_nodes, train_size, seed + 2)\n",
        "\n",
        "    test_dir = 'test'\n",
        "    test_loader = DataLoader(test_data, batch_size, False, num_workers=0)\n",
        "    out = validate(test_loader, actor, tsp.reward, tsp.render, test_dir, num_plot=5)\n",
        "\n",
        "    print('Average tour length: ', out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336c0c3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "336c0c3a",
        "outputId": "f7058c58-cdee-46c3-c21e-ecb35de1451a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "  Batch 99/3907, reward: 9.781, loss: -137.4742, took: 64.1935s\n",
            "  Batch 199/3907, reward: 7.863, loss: -0.4888, took: 63.5860s\n",
            "  Batch 299/3907, reward: 7.618, loss: -0.5283, took: 63.7242s\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    task = 'tsp' \n",
        "    \n",
        "    if task == 'tsp':\n",
        "        train_tsp()\n",
        "    else:\n",
        "        raise ValueError('Task <%s> not understood'%args.task)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff1db71",
      "metadata": {
        "id": "3ff1db71"
      },
      "outputs": [],
      "source": [
        "# parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8ef7cd",
      "metadata": {
        "id": "8d8ef7cd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "optim_rl_colab.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}